"""FunASR å¼•æ“å®ç°

åŸºäº FunASR å®ç°å®Œæ•´çš„è¯­éŸ³è¯†åˆ«åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆVADï¼‰
- è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰
- æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰
- è¯´è¯äººè¾¨åˆ«

æ”¯æŒç¦»çº¿éŸ³é¢‘æ–‡ä»¶å¤„ç†å’Œå®æ—¶éŸ³é¢‘æµå¤„ç†ã€‚
"""

import logging
import os
import tempfile
import time
from typing import Optional, Dict, Any, List
from pathlib import Path

from backend.utils.asr.base_engine import BaseASREngine

logger = logging.getLogger(__name__)


class FunASREngine(BaseASREngine):
    """
    FunASR å¼•æ“å®ç°
    
    æ”¯æŒå››å¤§æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. VADï¼ˆè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼‰- è¯†åˆ«æœ‰æ•ˆè¯­éŸ³æ®µ
    2. LIDï¼ˆè¯­è¨€è¯†åˆ«ï¼‰- è¯†åˆ«è¯­ç§å¹¶è½¬å†™æ–‡æœ¬
    3. SERï¼ˆæƒ…æ„Ÿè¯†åˆ«ï¼‰- è¯†åˆ«è¯­éŸ³æƒ…æ„Ÿ
    4. è¯´è¯äººè¾¨åˆ« - åŒºåˆ†ä¸åŒè¯´è¯äºº
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– FunASR å¼•æ“
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å‚æ•°ï¼š
                - sample_rate: é‡‡æ ·ç‡ï¼ˆé»˜è®¤ 16000ï¼‰
                - channels: å£°é“æ•°ï¼ˆé»˜è®¤ 1ï¼‰
                - sample_width: é‡‡æ ·ä½æ·±ï¼ˆé»˜è®¤ 2ï¼Œå³ 16-bitï¼‰
                - device: è®¾å¤‡ç±»å‹ "cpu" æˆ– "cuda:0"ï¼ˆé»˜è®¤ "cpu"ï¼‰
                - language: è¯­è¨€è®¾ç½® "auto", "zh", "en" ç­‰ï¼ˆé»˜è®¤ "auto"ï¼‰
                - min_audio_length: æœ€å°éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼Œé»˜è®¤ 1.0ï¼‰
                - vad_enabled: æ˜¯å¦å¯ç”¨ VADï¼ˆé»˜è®¤ Trueï¼‰
                - lid_enabled: æ˜¯å¦å¯ç”¨è¯­è¨€è¯†åˆ«ï¼ˆé»˜è®¤ Trueï¼‰
                - ser_enabled: æ˜¯å¦å¯ç”¨æƒ…æ„Ÿè¯†åˆ«ï¼ˆé»˜è®¤ Falseï¼‰
                - speaker_enabled: æ˜¯å¦å¯ç”¨è¯´è¯äººè¾¨åˆ«ï¼ˆé»˜è®¤ Falseï¼‰
                - model_cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
                - output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ "./funasr_output"ï¼‰
        """
        self.config = config
        
        # éŸ³é¢‘å‚æ•°
        self.sample_rate: int = config.get("sample_rate", 16000)
        self.channels: int = config.get("channels", 1)
        self.sample_width: int = config.get("sample_width", 2)
        self.min_audio_length: float = config.get("min_audio_length", 1.0)
        
        # è®¾å¤‡é…ç½®
        self.device: str = config.get("device", "cpu")
        self.language: str = config.get("language", "auto")
        
        # åŠŸèƒ½å¼€å…³
        self.vad_enabled: bool = config.get("vad_enabled", True)
        self.lid_enabled: bool = config.get("lid_enabled", True)
        self.ser_enabled: bool = config.get("ser_enabled", False)
        self.speaker_enabled: bool = config.get("speaker_enabled", False)
        
        # è·¯å¾„é…ç½®
        self.output_dir: str = config.get("output_dir", "./funasr_output")
        os.makedirs(self.output_dir, exist_ok=True)
        
        # æ¨¡å‹ç¼“å­˜é…ç½® - ç»Ÿä¸€ä½¿ç”¨ backend/data/asr ç›®å½•
        model_cache_dir = config.get("model_cache_dir")
        if model_cache_dir:
            os.environ["MODELSCOPE_CACHE"] = str(model_cache_dir)
            logger.info(f"è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•: {model_cache_dir}")
        
        # æ¨¡å‹å®ä¾‹
        self.vad_model = None
        self.lid_model = None
        self.ser_model = None
        self.speaker_model = None
        
        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = bytearray()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_processed": 0,
            "vad_triggers": 0,
            "transcripts": 0,
            "errors": 0
        }
    
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ– FunASR å¼•æ“ï¼ŒåŠ è½½æ‰€éœ€æ¨¡å‹
        
        Returns:
            bool: åˆå§‹åŒ–æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
        """
        try:
            # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨ä¸éœ€è¦æ—¶åŠ è½½
            from funasr import AutoModel
            
            logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– FunASR å¼•æ“...")
            logger.info(f"éŸ³é¢‘é…ç½®: {self.sample_rate}Hz, {self.channels}ch, {self.sample_width * 8}bit")
            logger.info(f"è®¾å¤‡: {self.device}, è¯­è¨€: {self.language}")
            
            # æ£€æŸ¥æ¨¡å‹ç¼“å­˜ç›®å½•
            model_cache_dir = self.config.get("model_cache_dir")
            if model_cache_dir and not self._check_models_exist(model_cache_dir):
                logger.error(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ python backend/all_ready.py ä¸‹è½½æ¨¡å‹")
                logger.error(f"é¢„æœŸæ¨¡å‹ç›®å½•: {model_cache_dir}")
                return False
            
            # 1. åŠ è½½ VAD æ¨¡å‹
            if self.vad_enabled:
                logger.info("åŠ è½½ VAD æ¨¡å‹ (fsmn-vad)...")
                start_time = time.time()
                self.vad_model = AutoModel(
                    model="fsmn-vad",
                    device=self.device,
                    disable_update=True
                )
                logger.info(f"âœ… VAD æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶ {time.time() - start_time:.2f}ç§’)")
            
            # 2. åŠ è½½è¯­è¨€è¯†åˆ«æ¨¡å‹ï¼ˆåŒ…å« ASR åŠŸèƒ½ï¼‰
            if self.lid_enabled:
                logger.info("åŠ è½½è¯­è¨€è¯†åˆ«æ¨¡å‹ (iic/SenseVoiceSmall)...")
                start_time = time.time()
                self.lid_model = AutoModel(
                    model="iic/SenseVoiceSmall",
                    trust_remote_code=True,
                    device=self.device,
                    disable_update=True
                )
                logger.info(f"âœ… è¯­è¨€è¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶ {time.time() - start_time:.2f}ç§’)")
            
            # 3. åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            if self.ser_enabled:
                # æ£€æŸ¥æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹æ˜¯å¦å­˜åœ¨
                ser_model_path = Path(model_cache_dir) / "models" / "iic" / "emotion2vec_plus_large"
                if not ser_model_path.exists():
                    logger.warning("âŒ æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå·²ç¦ç”¨æƒ…æ„Ÿè¯†åˆ«åŠŸèƒ½")
                    logger.warning("å¦‚éœ€ä½¿ç”¨æƒ…æ„Ÿè¯†åˆ«ï¼Œè¯·è¿è¡Œ: python backend/all_ready.py --download-emotion")
                    self.ser_enabled = False
                else:
                    logger.info("åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ (emotion2vec_plus_large)...")
                    start_time = time.time()
                    self.ser_model = AutoModel(
                        model="emotion2vec_plus_large",
                        device=self.device,
                        disable_update=True
                    )
                    logger.info(f"âœ… æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶ {time.time() - start_time:.2f}ç§’)")
            
            # 4. åŠ è½½è¯´è¯äººè¾¨åˆ«æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            if self.speaker_enabled:
                # æ£€æŸ¥è¯´è¯äººè¾¨åˆ«æ¨¡å‹æ˜¯å¦å­˜åœ¨
                speaker_model_path = Path(model_cache_dir) / "models" / "iic" / "speech_campplus_sv_zh-cn_16k-common"
                if not speaker_model_path.exists():
                    logger.warning("âŒ è¯´è¯äººè¾¨åˆ«æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå·²ç¦ç”¨è¯´è¯äººè¾¨åˆ«åŠŸèƒ½")
                    logger.warning("å¦‚éœ€ä½¿ç”¨è¯´è¯äººè¾¨åˆ«ï¼Œè¯·è¿è¡Œ: python backend/all_ready.py --download-speaker")
                    self.speaker_enabled = False
                else:
                    logger.info("åŠ è½½è¯´è¯äººè¾¨åˆ«æ¨¡å‹ (speech_campplus_sv_zh-cn_16k-common)...")
                    start_time = time.time()
                    self.speaker_model = AutoModel(
                        model="iic/speech_campplus_sv_zh-cn_16k-common",
                        trust_remote_code=True,
                        device=self.device,
                        disable_update=True
                    )
                    logger.info(f"âœ… è¯´è¯äººè¾¨åˆ«æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶ {time.time() - start_time:.2f}ç§’)")
            
            logger.info("âœ… FunASR å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except ImportError as e:
            logger.error(f"âŒ FunASR åº“æœªå®‰è£…: {e}")
            logger.error("è¯·è¿è¡Œ: pip install funasr")
            return False
        except Exception as e:
            logger.error(f"âŒ FunASR å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            return False
    
    async def process_audio(self, audio_data: bytes) -> Optional[str]:
        """
        å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œè¿›è¡Œè¯­éŸ³è¯†åˆ«
        
        Args:
            audio_data: PCM éŸ³é¢‘æ•°æ®
            
        Returns:
            Optional[str]: è¯†åˆ«å‡ºçš„æ–‡æœ¬ï¼Œå¦‚æœæ²¡æœ‰è¯†åˆ«ç»“æœè¿”å› None
        """
        try:
            # ç´¯ç§¯éŸ³é¢‘æ•°æ®
            self.audio_buffer.extend(audio_data)
            
            # è®¡ç®—ç¼“å†²åŒºæ—¶é•¿
            bytes_per_second = self.sample_rate * self.channels * self.sample_width
            buffer_duration = len(self.audio_buffer) / bytes_per_second
            
            # å¦‚æœç¼“å†²åŒºæœªè¾¾åˆ°æœ€å°é•¿åº¦ï¼Œç»§ç»­ç´¯ç§¯
            if buffer_duration < self.min_audio_length:
                return None
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            temp_audio_path = self._save_temp_audio(bytes(self.audio_buffer))
            
            # æ¸…ç©ºç¼“å†²åŒº
            self.audio_buffer.clear()
            
            # æ‰§è¡Œè¯†åˆ«
            result = await self._recognize_audio(temp_audio_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(temp_audio_path)
            except Exception:
                pass
            
            self.stats["total_processed"] += 1
            if result:
                self.stats["transcripts"] += 1
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†éŸ³é¢‘æ—¶å‡ºé”™: {e}", exc_info=True)
            self.stats["errors"] += 1
            return None
    
    async def detect_vad(self, audio_data: bytes) -> bool:
        """
        æ£€æµ‹è¯­éŸ³æ´»åŠ¨ï¼ˆVoice Activity Detectionï¼‰
        
        Args:
            audio_data: PCM éŸ³é¢‘æ•°æ®
            
        Returns:
            bool: æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨è¿”å› True
        """
        if not self.vad_enabled or not self.vad_model:
            return False
        
        try:
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            temp_audio_path = self._save_temp_audio(audio_data)
            
            # æ‰§è¡Œ VAD æ£€æµ‹
            result = self.vad_model.generate(input=temp_audio_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(temp_audio_path)
            except Exception:
                pass
            
            # è§£æ VAD ç»“æœ
            if result and len(result) > 0 and "value" in result[0]:
                vad_segments = result[0]["value"]
                # å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³æ®µï¼Œè¿”å› True
                if vad_segments and len(vad_segments) > 0:
                    self.stats["vad_triggers"] += 1
                    logger.debug(f"VAD æ£€æµ‹åˆ°è¯­éŸ³æ®µ: {vad_segments}")
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"VAD æ£€æµ‹æ—¶å‡ºé”™: {e}", exc_info=True)
            return False
    
    async def clear_buffer(self):
        """æ¸…ç©ºå†…éƒ¨éŸ³é¢‘ç¼“å†²åŒºï¼ˆç”¨äºæ‰“æ–­åœºæ™¯ï¼‰"""
        self.audio_buffer.clear()
        logger.debug("FunASR ç¼“å†²åŒºå·²æ¸…ç©º")
    
    async def shutdown(self):
        """å…³é—­ ASR å¼•æ“ï¼Œé‡Šæ”¾èµ„æº"""
        logger.info("æ­£åœ¨å…³é—­ FunASR å¼•æ“...")
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.audio_buffer.clear()
        
        # é‡Šæ”¾æ¨¡å‹ï¼ˆPython GC ä¼šå¤„ç†ï¼‰
        self.vad_model = None
        self.lid_model = None
        self.ser_model = None
        self.speaker_model = None
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"FunASR ç»Ÿè®¡: å¤„ç† {self.stats['total_processed']} æ¬¡, "
                   f"è½¬å½• {self.stats['transcripts']} æ¬¡, "
                   f"VAD è§¦å‘ {self.stats['vad_triggers']} æ¬¡, "
                   f"é”™è¯¯ {self.stats['errors']} æ¬¡")
        
        logger.info("âœ… FunASR å¼•æ“å·²å…³é—­")
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _check_models_exist(self, cache_dir: str) -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
        
        Args:
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            
        Returns:
            bool: æ¨¡å‹å­˜åœ¨è¿”å› True
        """
        cache_path = Path(cache_dir)
        if not cache_path.exists():
            logger.warning(f"ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_path}")
            return False
        
        # FunASR ä½¿ç”¨ models ç›®å½•ç»“æ„
        models_dir = cache_path / "models"
        if not models_dir.exists():
            logger.warning(f"models ç›®å½•ä¸å­˜åœ¨: {models_dir}")
            return False
        
        # æ£€æŸ¥å¿…éœ€çš„æ¨¡å‹ï¼ˆæ ¹æ®é…ç½®ï¼‰
        required_models = []
        optional_models = []
        
        # VAD æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.vad_enabled:
            required_models.append("iic/speech_fsmn_vad_zh-cn-16k-common-pytorch")
        
        # è¯­è¨€è¯†åˆ«æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.lid_enabled:
            required_models.append("iic/SenseVoiceSmall")
        
        # æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        if self.ser_enabled:
            optional_models.append("iic/emotion2vec_plus_large")
        
        # è¯´è¯äººè¾¨åˆ«æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        if self.speaker_enabled:
            optional_models.append("iic/speech_campplus_sv_zh-cn_16k-common")
        
        # æ£€æŸ¥å¿…éœ€æ¨¡å‹
        found_count = 0
        missing_models = []
        for model_id in required_models:
            parts = model_id.split("/")
            model_path = models_dir / parts[0] / parts[1]
            if model_path.exists():
                found_count += 1
                logger.debug(f"âœ“ æ‰¾åˆ°æ¨¡å‹: {model_id}")
            else:
                logger.warning(f"âœ— ç¼ºå¤±å¿…éœ€æ¨¡å‹: {model_id} (è·¯å¾„: {model_path})")
                missing_models.append(model_id)
        
        # æ£€æŸ¥å¯é€‰æ¨¡å‹
        for model_id in optional_models:
            parts = model_id.split("/")
            model_path = models_dir / parts[0] / parts[1]
            if model_path.exists():
                logger.debug(f"âœ“ æ‰¾åˆ°å¯é€‰æ¨¡å‹: {model_id}")
            else:
                logger.warning(f"âœ— ç¼ºå¤±å¯é€‰æ¨¡å‹: {model_id} (è·¯å¾„: {model_path})")
                missing_models.append(model_id)
        
        if found_count == len(required_models):
            logger.info(f"âœ… æ‰¾åˆ°æ‰€æœ‰å¿…éœ€æ¨¡å‹ ({found_count}/{len(required_models)})")
            if missing_models:
                logger.warning(f"ç¼ºå¤±å¯é€‰æ¨¡å‹: {', '.join(missing_models)}")
            return True
        else:
            logger.error(f"âŒ ç¼ºå¤±å¿…éœ€æ¨¡å‹ ({found_count}/{len(required_models)})")
            logger.error(f"ç¼ºå¤±çš„æ¨¡å‹: {', '.join(missing_models)}")
            return False
    
    def _save_temp_audio(self, audio_data: bytes) -> str:
        """
        å°†éŸ³é¢‘æ•°æ®ä¿å­˜ä¸ºä¸´æ—¶ WAV æ–‡ä»¶
        
        Args:
            audio_data: PCM éŸ³é¢‘æ•°æ®
            
        Returns:
            str: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        """
        import wave
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        fd, temp_path = tempfile.mkstemp(suffix=".wav", dir=self.output_dir)
        os.close(fd)
        
        # å†™å…¥ WAV æ–‡ä»¶
        with wave.open(temp_path, 'wb') as wav_file:
            wav_file.setnchannels(self.channels)
            wav_file.setsampwidth(self.sample_width)
            wav_file.setframerate(self.sample_rate)
            wav_file.writeframes(audio_data)
        
        return temp_path
    
    async def _recognize_audio(self, audio_path: str) -> Optional[str]:
        """
        å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œå®Œæ•´è¯†åˆ«ï¼ˆåŒ…æ‹¬ LIDã€SERã€è¯´è¯äººè¾¨åˆ«ï¼‰
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            Optional[str]: è¯†åˆ«ç»“æœæ–‡æœ¬
        """
        result_text = None
        
        try:
            # 1. è¯­è¨€è¯†åˆ« + è¯­éŸ³è½¬å†™ï¼ˆä¸»è¦åŠŸèƒ½ï¼‰
            if self.lid_enabled and self.lid_model:
                lid_result = self._recognize_language(audio_path)
                if lid_result["status"] == "success":
                    data = lid_result["data"]
                    language = data.get("language", "unknown")
                    text = data.get("text", "")
                    
                    if text:
                        result_text = text
                        logger.info(f"è¯†åˆ«ç»“æœ [{language}]: {text}")
            
            # 2. æƒ…æ„Ÿè¯†åˆ«ï¼ˆå¯é€‰ï¼‰
            if self.ser_enabled and self.ser_model and result_text:
                ser_result = self._recognize_emotion(audio_path)
                if ser_result["status"] == "success":
                    emotion = ser_result["data"].get("emotion", "unknown")
                    confidence = ser_result["data"].get("confidence", 0.0)
                    logger.info(f"æƒ…æ„Ÿè¯†åˆ«: {emotion} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            # 3. è¯´è¯äººè¾¨åˆ«ï¼ˆå¯é€‰ï¼‰
            if self.speaker_enabled and self.speaker_model:
                speaker_result = self._diarize_speakers(audio_path)
                if speaker_result["status"] == "success":
                    speakers = speaker_result["data"]
                    logger.info(f"è¯´è¯äººæ•°é‡: {len(set(s['speaker'] for s in speakers))}")
            
            return result_text
            
        except Exception as e:
            logger.error(f"è¯†åˆ«éŸ³é¢‘æ—¶å‡ºé”™: {e}", exc_info=True)
            return None
    
    def _recognize_language(self, audio_path: str) -> dict:
        """
        è¯­è¨€è¯†åˆ« + è¯­éŸ³è½¬å†™
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            dict: ç»“æ„åŒ–ç»“æœ
        """
        try:
            result = self.lid_model.generate(
                input=audio_path,
                language=self.language,
                use_itn=True  # æ–‡æœ¬è§„èŒƒåŒ–
            )
            
            lid_data = {
                "language": result[0].get("language", "unknown"),
                "text": result[0].get("text", "")
            }
            
            return {
                "status": "success",
                "data": lid_data,
                "msg": ""
            }
        except Exception as e:
            return {
                "status": "fail",
                "data": {},
                "msg": f"è¯­ç§è¯†åˆ«å¤±è´¥: {str(e)}"
            }
    
    def _recognize_emotion(self, audio_path: str) -> dict:
        """
        æƒ…æ„Ÿè¯†åˆ«
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            dict: ç»“æ„åŒ–ç»“æœ
        """
        try:
            result = self.ser_model.generate(
                input=audio_path,
                output_dir=self.output_dir,
                granularity="utterance",  # æ•´å¥çº§åˆ«
                extract_embedding=False
            )
            
            ser_data = {
                "emotion": result[0].get("emotion", "unknown"),
                "confidence": result[0].get("scores", 0.0)
            }
            
            return {
                "status": "success",
                "data": ser_data,
                "msg": ""
            }
        except Exception as e:
            return {
                "status": "fail",
                "data": {},
                "msg": f"æƒ…æ„Ÿè¯†åˆ«å¤±è´¥: {str(e)}"
            }
    
    def _diarize_speakers(self, audio_path: str) -> dict:
        """
        è¯´è¯äººè¾¨åˆ«
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            dict: ç»“æ„åŒ–ç»“æœ
        """
        try:
            result = self.speaker_model.generate(input=audio_path)
            
            speaker_data = []
            for item in result[0].get("value", []):
                speaker_data.append({
                    "speaker": item.get("spk", "unknown"),
                    "text": item.get("text", ""),
                    "start_time": item.get("start", 0),
                    "end_time": item.get("end", 0)
                })
            
            return {
                "status": "success",
                "data": speaker_data,
                "msg": ""
            }
        except Exception as e:
            return {
                "status": "fail",
                "data": [],
                "msg": f"è¯´è¯äººè¾¨åˆ«å¤±è´¥: {str(e)}"
            }
    
    # ==================== ç‹¬ç«‹åŠŸèƒ½æ¥å£ ====================
    
    async def vad_detect_file(self, audio_path: str) -> dict:
        """
        å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œ VAD æ£€æµ‹
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            dict: VAD æ£€æµ‹ç»“æœ
        """
        if not self.vad_model:
            return {"status": "fail", "data": [], "msg": "VAD æ¨¡å‹æœªåŠ è½½"}
        
        try:
            result = self.vad_model.generate(input=audio_path)
            vad_segments = result[0].get("value", []) if result else []
            
            return {
                "status": "success",
                "data": vad_segments,
                "msg": ""
            }
        except Exception as e:
            return {
                "status": "fail",
                "data": [],
                "msg": f"VAD æ£€æµ‹å¤±è´¥: {str(e)}"
            }
    
    async def recognize_file(self, audio_path: str) -> dict:
        """
        å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œå®Œæ•´è¯†åˆ«ï¼ˆLID + SER + è¯´è¯äººï¼‰
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            dict: å®Œæ•´è¯†åˆ«ç»“æœ
        """
        results = {}
        
        # VAD æ£€æµ‹
        if self.vad_enabled:
            results["vad"] = await self.vad_detect_file(audio_path)
        
        # è¯­è¨€è¯†åˆ«
        if self.lid_enabled and self.lid_model:
            results["lid"] = self._recognize_language(audio_path)
        
        # æƒ…æ„Ÿè¯†åˆ«
        if self.ser_enabled and self.ser_model:
            results["ser"] = self._recognize_emotion(audio_path)
        
        # è¯´è¯äººè¾¨åˆ«
        if self.speaker_enabled and self.speaker_model:
            results["speaker"] = self._diarize_speakers(audio_path)
        
        return results
