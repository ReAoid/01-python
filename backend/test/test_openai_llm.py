"""
OpenAI LLM æ¨¡å—æµ‹è¯•æ–‡ä»¶

æµ‹è¯• OpenAI LLM å®¢æˆ·ç«¯çš„å„é¡¹åŠŸèƒ½ï¼ˆæ¸…æ™°åˆ†é˜¶æ®µæµ‹è¯•ï¼‰:

é˜¶æ®µ 1: LLM åˆå§‹åŒ–æµ‹è¯•
é˜¶æ®µ 2: æ¶ˆæ¯è½¬æ¢æµ‹è¯•
é˜¶æ®µ 3: åŒæ­¥ç”Ÿæˆæµ‹è¯•
é˜¶æ®µ 4: å¼‚æ­¥ç”Ÿæˆæµ‹è¯•
é˜¶æ®µ 5: æµå¼ç”Ÿæˆæµ‹è¯•
é˜¶æ®µ 6: å¼‚æ­¥æµå¼ç”Ÿæˆæµ‹è¯•
é˜¶æ®µ 7: å‚æ•°é…ç½®æµ‹è¯•

ä½¿ç”¨æ–¹æ³•ï¼š
    cd /Users/mingy/Documents/python/01-python
    python backend/test/test_openai_llm.py

å‰ç½®æ¡ä»¶ï¼š
    - é…ç½®æœ‰æ•ˆçš„ LLM APIï¼ˆåœ¨ backend/config/settings.py ä¸­ï¼‰
    
æ³¨æ„ï¼š
    - æŸäº›æµ‹è¯•ä¼šå®é™…è°ƒç”¨ LLM APIï¼Œå¯èƒ½äº§ç”Ÿè´¹ç”¨
    - å¦‚æœ API é…ç½®æ— æ•ˆï¼Œéƒ¨åˆ†æµ‹è¯•ä¼šè¢«è·³è¿‡
"""

import asyncio
import sys
import logging
from pathlib import Path
from typing import List

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# -----------------------------------------------------------------------------
# è·¯å¾„é…ç½®
# -----------------------------------------------------------------------------
ROOT_DIR = Path(__file__).resolve().parent.parent.parent

# ç¡®ä¿ backend æ¨¡å—å¯ä»¥è¢«å¯¼å…¥
if str(ROOT_DIR) not in sys.path:
    sys.path.append(str(ROOT_DIR))

try:
    from backend.utils.openai_llm import OpenaiLlm
    from backend.core.message import Message, UserMessage, SystemMessage, AssistantMessage
    from backend.config import settings
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥å¿…è¦æ¨¡å—: {e}")
    print("è¯·æ£€æŸ¥é¡¹ç›®è·¯å¾„é…ç½®")
    sys.exit(1)


class OpenAILLMTester:
    """OpenAI LLM æµ‹è¯•ç±» - åˆ†é˜¶æ®µæµ‹è¯• LLM åŠŸèƒ½"""
    
    def __init__(self):
        self.llm = None
        self.api_available = False
        self.test_results = []
    
    def test_initialization(self) -> bool:
        """
        é˜¶æ®µ 1: LLM åˆå§‹åŒ–æµ‹è¯•
        
        æµ‹è¯•å†…å®¹ï¼š
        - ä½¿ç”¨é…ç½®åˆå§‹åŒ–
        - ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°åˆå§‹åŒ–
        - é…ç½®éªŒè¯
        """
        print("\n" + "="*60)
        print("é˜¶æ®µ 1: LLM åˆå§‹åŒ–æµ‹è¯•")
        print("="*60)
        
        try:
            # æµ‹è¯• 1.1: ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–
            print("\n[1.1] ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–...")
            try:
                llm = OpenaiLlm()
                self.llm = llm
                self.api_available = True
                print(f"âœ… LLM åˆå§‹åŒ–æˆåŠŸ")
                print(f"   æ¨¡å‹: {llm.model}")
                print(f"   å®¢æˆ·ç«¯: {'å·²åˆ›å»º' if llm.client else 'æœªåˆ›å»º'}")
                print(f"   å¼‚æ­¥å®¢æˆ·ç«¯: {'å·²åˆ›å»º' if llm.async_client else 'æœªåˆ›å»º'}")
            except ValueError as e:
                print(f"âš ï¸  LLM é…ç½®æ— æ•ˆ: {e}")
                print("   éƒ¨åˆ†æµ‹è¯•å°†è¢«è·³è¿‡")
                self.api_available = False
                return True  # é…ç½®é—®é¢˜ä¸ç®—æµ‹è¯•å¤±è´¥
            
            # æµ‹è¯• 1.2: ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°åˆå§‹åŒ–
            print("\n[1.2] æµ‹è¯•è‡ªå®šä¹‰å‚æ•°åˆå§‹åŒ–...")
            try:
                custom_llm = OpenaiLlm(
                    model="gpt-3.5-turbo",
                    api_key="test_key",
                    base_url="https://api.openai.com/v1",
                    timeout=30
                )
                assert custom_llm.model == "gpt-3.5-turbo"
                print("âœ… è‡ªå®šä¹‰å‚æ•°åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  è‡ªå®šä¹‰å‚æ•°åˆå§‹åŒ–æµ‹è¯•: {e}")
            
            print("\nâœ… é˜¶æ®µ 1 æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"\nâŒ é˜¶æ®µ 1 æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_message_conversion(self) -> bool:
        """
        é˜¶æ®µ 2: æ¶ˆæ¯è½¬æ¢æµ‹è¯•
        
        æµ‹è¯•å†…å®¹ï¼š
        - Message å¯¹è±¡è½¬æ¢ä¸º API æ ¼å¼
        - ä¸åŒç±»å‹æ¶ˆæ¯çš„è½¬æ¢
        - æ¶ˆæ¯åˆ—è¡¨è½¬æ¢
        """
        print("\n" + "="*60)
        print("é˜¶æ®µ 2: æ¶ˆæ¯è½¬æ¢æµ‹è¯•")
        print("="*60)
        
        if not self.api_available or not self.llm:
            print("âš ï¸  LLM æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            return True
        
        try:
            # æµ‹è¯• 2.1: å•ä¸ªæ¶ˆæ¯è½¬æ¢
            print("\n[2.1] å•ä¸ªæ¶ˆæ¯è½¬æ¢...")
            user_msg = UserMessage("Hello")
            converted = self.llm._convert_messages([user_msg])
            assert len(converted) == 1
            assert converted[0]["role"] == "user"
            assert converted[0]["content"] == "Hello"
            print(f"âœ… å•ä¸ªæ¶ˆæ¯è½¬æ¢æˆåŠŸ: {converted[0]}")
            
            # æµ‹è¯• 2.2: å¤šä¸ªæ¶ˆæ¯è½¬æ¢
            print("\n[2.2] å¤šä¸ªæ¶ˆæ¯è½¬æ¢...")
            messages = [
                SystemMessage("You are a helpful assistant"),
                UserMessage("What is Python?"),
                AssistantMessage("Python is a programming language")
            ]
            converted = self.llm._convert_messages(messages)
            assert len(converted) == 3
            assert converted[0]["role"] == "system"
            assert converted[1]["role"] == "user"
            assert converted[2]["role"] == "assistant"
            print(f"âœ… å¤šä¸ªæ¶ˆæ¯è½¬æ¢æˆåŠŸ ({len(converted)} æ¡)")
            for i, msg in enumerate(converted):
                print(f"   [{i+1}] {msg['role']}: {msg['content'][:30]}...")
            
            print("\nâœ… é˜¶æ®µ 2 æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"\nâŒ é˜¶æ®µ 2 æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_sync_generation(self) -> bool:
        """
        é˜¶æ®µ 3: åŒæ­¥ç”Ÿæˆæµ‹è¯•
        
        æµ‹è¯•å†…å®¹ï¼š
        - åŸºæœ¬ç”Ÿæˆè°ƒç”¨
        - è¿”å›æ¶ˆæ¯æ ¼å¼
        - ç®€å•å¯¹è¯
        """
        print("\n" + "="*60)
        print("é˜¶æ®µ 3: åŒæ­¥ç”Ÿæˆæµ‹è¯•")
        print("="*60)
        
        if not self.api_available or not self.llm:
            print("âš ï¸  LLM æœªåˆå§‹åŒ–æˆ– API ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            return True
        
        try:
            # æµ‹è¯• 3.1: ç®€å•é—®ç­”
            print("\n[3.1] æµ‹è¯•ç®€å•é—®ç­”...")
            print("   å‘é€: 'Say hello in Chinese'")
            messages = [
                SystemMessage("You are a helpful assistant. Be concise."),
                UserMessage("Say hello in Chinese")
            ]
            
            try:
                response = self.llm.generate(messages)
                assert isinstance(response, Message)
                assert response.role == "assistant"
                assert len(response.content) > 0
                print(f"âœ… ç”ŸæˆæˆåŠŸ")
                print(f"   å“åº”: {response.content[:100]}...")
            except Exception as e:
                print(f"âš ï¸  API è°ƒç”¨å¤±è´¥: {e}")
                print("   è¿™å¯èƒ½æ˜¯ç½‘ç»œæˆ– API é…ç½®é—®é¢˜")
                return True  # API è°ƒç”¨å¤±è´¥ä¸ç®—æµ‹è¯•å¤±è´¥
            
            # æµ‹è¯• 3.2: å¸¦æ¸©åº¦å‚æ•°çš„ç”Ÿæˆ
            print("\n[3.2] æµ‹è¯•å¸¦æ¸©åº¦å‚æ•°çš„ç”Ÿæˆ...")
            messages = [UserMessage("Pick a number between 1 and 10")]
            try:
                response = self.llm.generate(messages, temperature=0.0)
                print(f"âœ… å¸¦å‚æ•°ç”ŸæˆæˆåŠŸ")
                print(f"   å“åº”: {response.content[:100]}...")
            except Exception as e:
                print(f"âš ï¸  API è°ƒç”¨å¤±è´¥: {e}")
                return True
            
            print("\nâœ… é˜¶æ®µ 3 æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"\nâŒ é˜¶æ®µ 3 æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def test_async_generation(self) -> bool:
        """
        é˜¶æ®µ 4: å¼‚æ­¥ç”Ÿæˆæµ‹è¯•
        
        æµ‹è¯•å†…å®¹ï¼š
        - å¼‚æ­¥ç”Ÿæˆè°ƒç”¨
        - å¼‚æ­¥è¿”å›æ ¼å¼
        """
        print("\n" + "="*60)
        print("é˜¶æ®µ 4: å¼‚æ­¥ç”Ÿæˆæµ‹è¯•")
        print("="*60)
        
        if not self.api_available or not self.llm:
            print("âš ï¸  LLM æœªåˆå§‹åŒ–æˆ– API ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            return True
        
        try:
            # æµ‹è¯• 4.1: å¼‚æ­¥é—®ç­”
            print("\n[4.1] æµ‹è¯•å¼‚æ­¥é—®ç­”...")
            print("   å‘é€: 'What is 2+2? Answer only with the number.'")
            messages = [UserMessage("What is 2+2? Answer only with the number.")]
            
            try:
                response = await self.llm.agenerate(messages)
                assert isinstance(response, Message)
                assert response.role == "assistant"
                assert len(response.content) > 0
                print(f"âœ… å¼‚æ­¥ç”ŸæˆæˆåŠŸ")
                print(f"   å“åº”: {response.content[:100]}...")
            except Exception as e:
                print(f"âš ï¸  API è°ƒç”¨å¤±è´¥: {e}")
                return True
            
            print("\nâœ… é˜¶æ®µ 4 æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"\nâŒ é˜¶æ®µ 4 æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_stream_generation(self) -> bool:
        """
        é˜¶æ®µ 5: æµå¼ç”Ÿæˆæµ‹è¯•
        
        æµ‹è¯•å†…å®¹ï¼š
        - æµå¼ç”Ÿæˆè°ƒç”¨
        - æµå¼è¾“å‡ºå¤„ç†
        - å®Œæ•´å“åº”ç»„è£…
        """
        print("\n" + "="*60)
        print("é˜¶æ®µ 5: æµå¼ç”Ÿæˆæµ‹è¯•")
        print("="*60)
        
        if not self.api_available or not self.llm:
            print("âš ï¸  LLM æœªåˆå§‹åŒ–æˆ– API ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            return True
        
        try:
            # æµ‹è¯• 5.1: æµå¼é—®ç­”
            print("\n[5.1] æµ‹è¯•æµå¼é—®ç­”...")
            print("   å‘é€: 'Count from 1 to 5'")
            messages = [
                SystemMessage("You are a helpful assistant. Be concise."),
                UserMessage("Count from 1 to 5")
            ]
            
            try:
                print("   æ¥æ”¶æµå¼è¾“å‡º: ", end="", flush=True)
                full_response = ""
                chunk_count = 0
                
                for chunk in self.llm.stream(messages):
                    full_response += chunk
                    chunk_count += 1
                    print(".", end="", flush=True)
                
                print(f" å®Œæˆ")
                print(f"âœ… æµå¼ç”ŸæˆæˆåŠŸ")
                print(f"   æ¥æ”¶ {chunk_count} ä¸ªæ•°æ®å—")
                print(f"   å®Œæ•´å“åº”: {full_response[:100]}...")
                
                assert chunk_count > 0, "åº”è¯¥æ¥æ”¶åˆ°è‡³å°‘ä¸€ä¸ªæ•°æ®å—"
                assert len(full_response) > 0, "å®Œæ•´å“åº”ä¸åº”ä¸ºç©º"
                
            except Exception as e:
                print(f"\nâš ï¸  API è°ƒç”¨å¤±è´¥: {e}")
                return True
            
            print("\nâœ… é˜¶æ®µ 5 æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"\nâŒ é˜¶æ®µ 5 æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def test_async_stream_generation(self) -> bool:
        """
        é˜¶æ®µ 6: å¼‚æ­¥æµå¼ç”Ÿæˆæµ‹è¯•
        
        æµ‹è¯•å†…å®¹ï¼š
        - å¼‚æ­¥æµå¼ç”Ÿæˆ
        - å¼‚æ­¥è¿­ä»£å¤„ç†
        """
        print("\n" + "="*60)
        print("é˜¶æ®µ 6: å¼‚æ­¥æµå¼ç”Ÿæˆæµ‹è¯•")
        print("="*60)
        
        if not self.api_available or not self.llm:
            print("âš ï¸  LLM æœªåˆå§‹åŒ–æˆ– API ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            return True
        
        try:
            # æµ‹è¯• 6.1: å¼‚æ­¥æµå¼é—®ç­”
            print("\n[6.1] æµ‹è¯•å¼‚æ­¥æµå¼é—®ç­”...")
            print("   å‘é€: 'Say hi'")
            messages = [UserMessage("Say hi")]
            
            try:
                print("   æ¥æ”¶å¼‚æ­¥æµå¼è¾“å‡º: ", end="", flush=True)
                full_response = ""
                chunk_count = 0
                
                async for chunk in self.llm.astream(messages):
                    full_response += chunk
                    chunk_count += 1
                    print(".", end="", flush=True)
                
                print(f" å®Œæˆ")
                print(f"âœ… å¼‚æ­¥æµå¼ç”ŸæˆæˆåŠŸ")
                print(f"   æ¥æ”¶ {chunk_count} ä¸ªæ•°æ®å—")
                print(f"   å®Œæ•´å“åº”: {full_response[:100]}...")
                
                assert chunk_count > 0, "åº”è¯¥æ¥æ”¶åˆ°è‡³å°‘ä¸€ä¸ªæ•°æ®å—"
                assert len(full_response) > 0, "å®Œæ•´å“åº”ä¸åº”ä¸ºç©º"
                
            except Exception as e:
                print(f"\nâš ï¸  API è°ƒç”¨å¤±è´¥: {e}")
                return True
            
            print("\nâœ… é˜¶æ®µ 6 æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"\nâŒ é˜¶æ®µ 6 æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_parameter_configuration(self) -> bool:
        """
        é˜¶æ®µ 7: å‚æ•°é…ç½®æµ‹è¯•
        
        æµ‹è¯•å†…å®¹ï¼š
        - æ¸©åº¦å‚æ•°
        - æœ€å¤§ token æ•°
        - å…¶ä»–é…ç½®å‚æ•°
        """
        print("\n" + "="*60)
        print("é˜¶æ®µ 7: å‚æ•°é…ç½®æµ‹è¯•")
        print("="*60)
        
        if not self.api_available or not self.llm:
            print("âš ï¸  LLM æœªåˆå§‹åŒ–æˆ– API ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            return True
        
        try:
            # æµ‹è¯• 7.1: ä¸åŒæ¸©åº¦å‚æ•°
            print("\n[7.1] æµ‹è¯•æ¸©åº¦å‚æ•° (temperature=0.0)...")
            messages = [UserMessage("Say 'test'")]
            
            try:
                response = self.llm.generate(messages, temperature=0.0)
                print(f"âœ… temperature=0.0 è°ƒç”¨æˆåŠŸ")
                print(f"   å“åº”: {response.content[:50]}...")
            except Exception as e:
                print(f"âš ï¸  API è°ƒç”¨å¤±è´¥: {e}")
                return True
            
            # æµ‹è¯• 7.2: max_tokens å‚æ•°
            print("\n[7.2] æµ‹è¯• max_tokens å‚æ•°...")
            try:
                response = self.llm.generate(
                    messages,
                    max_tokens=10,
                    temperature=1.0
                )
                print(f"âœ… max_tokens=10 è°ƒç”¨æˆåŠŸ")
                print(f"   å“åº”: {response.content}")
            except Exception as e:
                print(f"âš ï¸  API è°ƒç”¨å¤±è´¥: {e}")
                return True
            
            print("\nâœ… é˜¶æ®µ 7 æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"\nâŒ é˜¶æ®µ 7 æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*60)
    print("ğŸš€ OpenAI LLM æ¨¡å—æµ‹è¯•")
    print("="*60)
    print("\nâš ï¸  æ³¨æ„: æŸäº›æµ‹è¯•ä¼šå®é™…è°ƒç”¨ LLM APIï¼Œå¯èƒ½äº§ç”Ÿè´¹ç”¨")
    print("å¦‚æœä¸æƒ³è°ƒç”¨ APIï¼Œå¯ä»¥åœ¨é…ç½®ä¸­è®¾ç½®æ— æ•ˆçš„ API å¯†é’¥\n")
    
    tester = OpenAILLMTester()
    
    try:
        # æŒ‰é¡ºåºæ‰§è¡Œæµ‹è¯•
        sync_tests = [
            ("LLM åˆå§‹åŒ–", tester.test_initialization),
            ("æ¶ˆæ¯è½¬æ¢", tester.test_message_conversion),
            ("åŒæ­¥ç”Ÿæˆ", tester.test_sync_generation),
            ("æµå¼ç”Ÿæˆ", tester.test_stream_generation),
            ("å‚æ•°é…ç½®", tester.test_parameter_configuration),
        ]
        
        async_tests = [
            ("å¼‚æ­¥ç”Ÿæˆ", tester.test_async_generation),
            ("å¼‚æ­¥æµå¼ç”Ÿæˆ", tester.test_async_stream_generation),
        ]
        
        results = []
        
        # æ‰§è¡ŒåŒæ­¥æµ‹è¯•
        for test_name, test_func in sync_tests:
            result = test_func()
            results.append((test_name, result))
        
        # æ‰§è¡Œå¼‚æ­¥æµ‹è¯•
        for test_name, test_func in async_tests:
            result = await test_func()
            results.append((test_name, result))
        
        # æ‰“å°æµ‹è¯•æ€»ç»“
        print("\n" + "="*60)
        print("æµ‹è¯•æ€»ç»“")
        print("="*60)
        
        passed = sum(1 for _, result in results if result)
        total = len(results)
        
        for test_name, result in results:
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"{test_name}: {status}")
        
        print("="*60)
        print(f"æ€»è®¡: {passed}/{total} é€šè¿‡")
        
        if not tester.api_available:
            print("\nâš ï¸  LLM API æœªé…ç½®æˆ–ä¸å¯ç”¨ï¼Œéƒ¨åˆ†æµ‹è¯•è¢«è·³è¿‡")
            print("é…ç½®æœ‰æ•ˆçš„ API ä»¥è¿è¡Œå®Œæ•´æµ‹è¯•")
        
        if passed == total:
            print("âœ¨ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            print(f"âš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
            return False
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
